{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN training and testing for MitoSplit-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mIcpL6pLPvL"
   },
   "source": [
    "### Import required Python libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaV4mHBXVj2x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('axes', labelsize=20)\n",
    "plt.rc('legend', fontsize=18)\n",
    "import napari\n",
    "import skimage as sk\n",
    "import skimage.segmentation\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "import h5py   # HDF5 data file management library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Activation\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D\n",
    "from tensorflow.python.keras.layers import concatenate, UpSampling2D, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVMwTVUOV1KY"
   },
   "source": [
    "### Import and reshape data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "lxMLelekV8xh",
    "outputId": "d24c25ea-08f4-4a95-b467-f3be7088cb50"
   },
   "outputs": [],
   "source": [
    "data_path = 'Y:/Model/'  # nb: begin with /\n",
    "labels = ['input_data1', 'input_data2', 'output_data']\n",
    "\n",
    "#Inputs\n",
    "#Mito\n",
    "input_data_filename1 = data_path + 'Mito.h5' \n",
    "input_data1 = np.array(h5py.File(input_data_filename1, 'r').get('Mito'))\n",
    "input_data1 = input_data1.reshape((*input_data1.shape, 1))\n",
    "print(labels[0]+':', input_data1.shape)\n",
    "\n",
    "#Drp1\n",
    "input_data_filename2 = data_path + 'Drp1.h5' \n",
    "input_data2 = np.array(h5py.File(input_data_filename2, 'r').get('Drp1'))\n",
    "input_data2 = input_data2.reshape((*input_data2.shape, 1))\n",
    "print(labels[1]+':', input_data1.shape)\n",
    "\n",
    "#Concatenate channels\n",
    "input_data = np.concatenate((input_data1, input_data2), axis=3)\n",
    "input_data= input_data.reshape((*input_data.shape, 1))\n",
    "print('Inputs:', input_data.shape)\n",
    "\n",
    "#Outputs\n",
    "output_data_filename = data_path + 'Proc.h5'\n",
    "output_data = np.array(h5py.File(output_data_filename, 'r').get('Proc'))\n",
    "output_data = output_data.reshape((*output_data.shape, 1, 1))\n",
    "print('Outputs:', output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blending = 'additive'\n",
    "opacity = 0.7\n",
    "\n",
    "cmap_mito = 'gray'\n",
    "cmap_drp1 = 'green'\n",
    "\n",
    "cmap_output = 'red'\n",
    "gamma_output = 0.2\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(input_data1.reshape(input_data1.shape[:-1]), colormap=cmap_mito, \n",
    "                 blending=blending, opacity=opacity, name='Mito')\n",
    "viewer.add_image(input_data2.reshape(input_data2.shape[:-1]), colormap=cmap_drp1, \n",
    "                 blending=blending, opacity=opacity, name='Drp1')\n",
    "viewer.add_image(output_data.reshape(output_data.shape[:-2]), colormap=cmap_output, \n",
    "                 blending=blending, gamma=gamma_output, name='Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = [50, 500, 800]\n",
    "cmap = 'gray'\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(5*3, 5))\n",
    "for ax, i in zip(axes, item_id):\n",
    "  ax.imshow(data[i].reshape(data.shape[1:3]), cmap=cmap)\n",
    "  ax.set_title('input_data1', size=20)\n",
    "  ax.set(xticks=[], yticks=[])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rkaaw6D6LjLU"
   },
   "source": [
    "## Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "pAolzClhLjT8",
    "outputId": "6f2fa10c-de9c-4623-ff70-8ad218c092ae"
   },
   "outputs": [],
   "source": [
    "# Split data set into [test] and [train+valid] subsets using sklearn train_test_split function\n",
    "\n",
    "data_set_test_trainvalid_ratio = 0.2\n",
    "data_split_state = None   # integer (usually 42) or None for random split on each call\n",
    "input_train, input_test, output_train, output_test =  train_test_split(input_data, \n",
    "                                                                       output_data, \n",
    "                                                                       test_size=data_set_test_trainvalid_ratio, \n",
    "                                                                       random_state=data_split_state)\n",
    "\n",
    "print('test:[train+valid] split ratio : ', data_set_test_trainvalid_ratio)\n",
    "print('data_split_state : ', data_split_state)\n",
    "print()\n",
    "\n",
    "print('input_data : ', input_data.shape, input_data.dtype)\n",
    "print('input_train : ', input_train.shape, input_train.dtype)\n",
    "print('input_test : ', input_test.shape, input_test.dtype)\n",
    "print()\n",
    "print('output_data : ', output_data.shape, output_data.dtype)\n",
    "print('output_train : ', output_train.shape, output_train.dtype)\n",
    "print('output_test : ', output_test.shape, output_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgB--wW0dGPU"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdlhlPRPdZck"
   },
   "outputs": [],
   "source": [
    "optimizer_type = Adam(learning_rate=0.5e-3)  # optimisation algorithm: Adam \n",
    "loss = 'mean_squared_error'  # loss (cost) function to be minimised by the optimiser\n",
    "metrics = ['mean_absolute_error']  # network accuracy metric to be determined after each epoch\n",
    "validtrain_split_ratio = 0.2  # % of the seen dataset to be put aside for validation, rest is for training\n",
    "max_epochs = 20  # maxmimum number of epochs to be iterated\n",
    "batch_size = 256   # batch size for the training data set\n",
    "batch_shuffle = True   # shuffle the training data prior to batching before each epoch\n",
    "\n",
    "nb_filters = 8\n",
    "firstConvSize = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6JHphuLcvb6"
   },
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVOQwMTkdrdb"
   },
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 2,1)\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# encoder section\n",
    "\n",
    "down0 = Conv3D(nb_filters, (firstConvSize, firstConvSize, 2), padding='same')(inputs)\n",
    "down0 = BatchNormalization()(down0)\n",
    "down0 = Activation('relu')(down0)\n",
    "down0 = Conv3D(nb_filters, (firstConvSize, firstConvSize, 2), padding='same')(down0)\n",
    "down0 = BatchNormalization()(down0)\n",
    "down0 = Activation('relu')(down0)\n",
    "down0_pool = MaxPooling3D((2, 2, 2), strides=(2, 2, 2))(down0)\n",
    "down0_pool = Reshape((64, 64, nb_filters))(down0_pool)\n",
    "down0 = Reshape((128,128,nb_filters*2))(down0)\n",
    "    \n",
    "down1 = Conv2D(nb_filters*2, (3, 3), padding='same')(down0_pool)\n",
    "down1 = BatchNormalization()(down1)\n",
    "down1 = Activation('relu')(down1)\n",
    "down1 = Conv2D(nb_filters*2, (3, 3), padding='same')(down1)\n",
    "down1 = BatchNormalization()(down1)\n",
    "down1 = Activation('relu')(down1)\n",
    "down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "    \n",
    "# center section\n",
    "    \n",
    "center = Conv2D(nb_filters*4, (3, 3), padding='same')(down1_pool)\n",
    "center = BatchNormalization()(center)\n",
    "center = Activation('relu')(center)\n",
    "center = Conv2D(nb_filters*4, (3, 3), padding='same')(center)\n",
    "center = BatchNormalization()(center)\n",
    "center = Activation('relu')(center)\n",
    "    \n",
    "# decoder section with skip connections to the encoder section\n",
    "\n",
    "up1 = UpSampling2D((2, 2))(center)\n",
    "up1 = concatenate([down1, up1], axis=3)\n",
    "up1 = Conv2D(nb_filters*2, (3, 3), padding='same')(up1)\n",
    "up1 = BatchNormalization()(up1)\n",
    "up1 = Activation('relu')(up1)\n",
    "up1 = Conv2D(nb_filters*2, (3, 3), padding='same')(up1)\n",
    "up1 = BatchNormalization()(up1)\n",
    "up1 = Activation('relu')(up1)\n",
    "\n",
    "up0 = UpSampling2D((2, 2))(up1)\n",
    "up0 = concatenate([down0, up0], axis=3)\n",
    "up0 = Conv2D(nb_filters, (3, 3), padding='same')(up0)\n",
    "up0 = BatchNormalization()(up0)\n",
    "up0 = Activation('relu')(up0)\n",
    "up0 = Conv2D(nb_filters, (3, 3), padding='same')(up0)\n",
    "up0 = BatchNormalization()(up0)\n",
    "up0 = Activation('relu')(up0)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='relu')(up0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwllZ6zVd9QM"
   },
   "source": [
    "### Compile the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8ngMDyUFeCDL",
    "outputId": "ad5736bf-3167-45bc-d413-c8d8ea7342f3"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=optimizer_type, loss=loss, metrics=metrics)\n",
    "print(model.summary())  \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvfyQk9LgaEz"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "qmXxAsVZggoD",
    "outputId": "9bda0248-fc33-4f01-912e-47e6132df8da"
   },
   "outputs": [],
   "source": [
    "print('Training initiated')\n",
    "print()\n",
    "\n",
    "history = model.fit(input_train, output_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=max_epochs,\n",
    "                    validation_split=validtrain_split_ratio,\n",
    "                    shuffle=batch_shuffle)\n",
    "\n",
    "print()\n",
    "print('Training completed')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20201014_Mito2Drp1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
